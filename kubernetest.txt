# self register node by kubelet( the one make sure workload run on pod, each naode has one)
--register-node=true

#manullay register node
--register-node=false

#prevent kube scheduler(one create & assign pod to nodes) from creating further nodes
kubectl cordon <node-name>

#status of node
kubectl describe node <node-name>

#create or update deployment using manifest file
kubectl apply -f <path to mainfest file>

#create deployment using imperative command
kubectl create deployment <deployment name> --image <image name>
#ex: kubectl create deployment nginx --image nginx

#create deployment (object) using imperative object configuration
kubectl create -f <manifest file>

#delete deployment (object) using imperative object configuration
kubectl delete -f <manifest file>

# delete object  with child resources first
kubectl delete deployment nginx-deployment --cascade=foreground

# delete object  immediately without deleting child resource, which is cleaned up by system later
kubectl delete deployment nginx-deployment --cascade=background

# delete object & retain child resources orphaned
kubectl delete deployment nginx-deployment --cascade=orphan

#update deployment (object) using imperative object configuration
kubectl replace -f <manifest file>

# deployment (object) using declarative object configuration
kubectl diff -f <folder path> #shows difference between adlready running config and new specified file
kubectl apply -f <folder path> # add -R to both as argument do it recursively

# equality based labels
  ex: prod = dev
  ex: tier != frontend

# set based labels
 ex: environment in (production, qa) # enviroment key with value 'qa' or 'production'
 ex: tier notin (frontend, backend) # tier key with 'frontend' & 'backend'
 ex: !partion

#get pods using equality based label selector
kubectl get pods -l environment=production,tier=frontend # ',' reperesents and condition

#get pods using set based label selector
kubectl get pods -l 'environment in (production, qa)'

#To see the Deployment rollout status
kubectl rollout status deployment/<deployment name>

#Get details of your object
kubectl describe <object name>

#chaning two operations
kubectl get $(kubectl -f docs/concepts/cluster-administration/nginx/ -o name | grep service/)

#get  status of a deployemnt/resource
kubectl rollout status <object type>/<object name>

#scaling a deployement
kubectl scale deployement/<deployement name> --replicas=<number>

#autoscale a deployment
kubectl autoscale deployement/<deployement name> --max=<number> --min=<number> --cpu-percent=<percentage>

#rollback to previous version
kubectl rollback undo <object type>/<object name>

# get history of rollout
kubectl rollout history <object type>/<object name>

#update label for a kind of object
kubectl label <object kind> -l <key>=<value>

#create namespace
kubectl create namespace prod

#get namespaces
kubectl get namespace

#To see which Kubernetes resources are and aren't in a namespace:
kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false

#set namespace
ex: kubectl get pods --namespace=<name>

# annotations: metadata not used for filtering, include general infor like github repo, author etc
 ex: apiVersion: v1
     kind: Pod
     metadata:
     name: annotations-demo
       annotations:
         imageregistry: "https://hub.docker.com/

#get object by fiels selectors
kubectl get <object type> --field-selector <kind.property><operator><value>
#ex: kubectl get pods --field-selector metadata.namespace!=default

#finalizers: condition that need to be chacked and emptied before deleting a resources
 ex: metadata:
       finalizers:
       - Kubernetes

# set owner for an object
ex: metadata:
      ownerReferences: <object name>


#Job is a Kubernetes resource that runs a Pod
#job conntroller cretes and delete pods by sending commands to api-server
#lease object contains node heartbeats, kubelet update the spec.renewTime property in lease object about heartbeat
#container runtime is software responsible for running containers
# <imagePullPolicy> pulls image from registry. values are always, never& ifNotPresent
# set <serializeImagePulls> to false so that 2 nodes call pull images parallel
# probe is a diagnostic performed by kubelet on container
# Lifecycle: This refers to specific hooks that allow you to execute code at certain points in a container’s lifecycle. There are two main hooks:
#postStart: This hook is executed immediately after a container is created. However, there is no guarantee that the hook will execute before the container’s entrypoint
# preStop: This hook is called immediately before a container is terminated. It can be used to perform cleanup or graceful shutdown operations.
# LivenessProbe: This is used to check the health of a container. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restartPolicy
# ReadinessProbe: This indicates whether a container is ready to serve requests. If the readiness probe fails, the endpoints controller removes the Pod’s IP address from the endpoints of all Services that match the Pod. This is a way to ensure that traffic does not reach a container that isn’t ready for it.
# StartupProbe: This is used to check if an application within a container has started. If the startup probe fails, the kubelet will kill the container, and the container will be subjected to its restartPolicy. Startup probes are useful for slow-starting containers, ensuring that the kubelet doesn’t kill them before they are up and running.
# initContainer: runs before pods app/main container, probes are not supported.sequentially executed if there is multiple init, when fails it behaves w.r.t restartPolicy in pods definition
# sidecarContainer: runs long with app/main container, probes are  supported. Has seperate restart policy, cna be sperateley stopeed. started or restarted from app conatiner. used for logging,dta sync, secuirty monitoring
# ephermal conatiner: temporary container for troubleshooting, not bor running app, probes, port are not supported.
# podDisruptionBudget ensures min number of pod runs even we try to remmove all of them
# pod Quality Of Service: 3 types: guaranteed, burstable and bestEffort- evited in te order- bestEffort(no container in that pod has memory or cpu req/limit), burstable(any of the container in that pod has memory or cpu req/limit), guaranteed(all container has same  memory or cpu req/limit)
# downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.
# user Namespaces isolate each pod from each other from the host, so the user in a container doesn't have any privileges like roor user in host other pods.
# A DaemonSet ensures that all (or some) Nodes run a copy of a Pod
# A CronJob creates Jobs on a repeating schedule
# Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster
# gateway api is same as ingress but ca route traffic based on tcp/udp protocol while ingress only route traffice of http/https
# A ReplicationController ensures that a specified number of pod replicas are running at any one time.
# endpointSlices is used to tract network endpoints related to a service which indeed represent IP addresses and ports of the Pods that are part of the Service
# format of dns name of a pod: pod-ipv4-address.my-namespace.pod.cluster-domain.example. For example, if a Pod in the default namespace has the IP address 172.17.0.3, and the domain name for your cluster is cluster.local, then the Pod has a DNS name: 172-17-0-3.default.pod.cluster.local
# Pod with spec.hostname set to "foo", and spec.subdomain set to "bar", in namespace "my-namespace", will have its hostname set to "foo" and its fully qualified domain name (FQDN) set to "foo.bar.my-namespace.svc.cluster.local"
# spec.ipFamilyPolicy can set the ip address of a pod or service as single or dualstack, poosible values: SingleStack, PreferDualStack, RequireDualStack
# .spec.ipFamilies accepts array which set whether ipv4 or 6 or both neeeds to assign ot pod or service. ex:["IPv4"],["IPv6"],["IPv4","IPv6"]
# internal-only traffic policy: If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use Service Internal Traffic Policy to keep network traffic within that node
# A ConfigMap provides a way to inject configuration data into pods.
# volume type:
#   emptyDir: A temporary directory that is created when a pod is assigned to a node and exists as long as the pod is running
#   hostPath: Mounts a file or directory from the host node’s filesystem into a pod
#   persistentVolume: Suitable for long-term storage needs
#   NFS: Allows multiple pods to share the same storage, which is mounted from a network file system
#   configMaps & secrets: Provides a way to inject configuration data and sensitive information into pods
#   CSI (Container Storage Interface): Allows for the use of third-party storage solutions.
#   image: olume source represents an OCI object (a container image or artifact) which is available on the kubelet's host machine.
#   local: volume represents a mounted local storage device such as a disk, partition or directory
# storageClass: administrators to define different types of storage they offer within a cluster like azure files, aws EBS etc.
# VolumeSnapshotContent: is a snapshot taken from a volume in the cluster that has been provisioned by an administrator
# VolumeSnapshot: is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim
# A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
# A file that is used to configure access to clusters is called a kubeconfig file
# Limit Ranges: are policies used to constrain resource allocations (like CPU and memory) within a namespace.
# Pod Overhead: is a way to account for the resources consumed by the Pod infrastructure on top of the container requests & limits.
# Taints: are applied to nodes to mark them as having special properties that should not accept any pods that do not tolerate the taints.
# Tolerations: are applied to pods to allow them to be scheduled on nodes with matching taints
# ExternalName service is a special type of service that maps a service name to an external DNS name, rather than routing traffic to a set of pods.
